{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction","text":"<p>Curve fitting is one of the most powerful tools available to us in modern computing. Curve fitting describes the process of matching multiple data points with an equation while finding values to describe the equation's parameters as well as their margins of error. </p> <p>In this lesson, we're going to demonstrate how to perform a curve fit using data taken at the University of Colorado Denver from a gamma spectroscopy lab. The data will include 6 radioactive isotopes with known emission spectra as well as a 7th unknown radioactive source. In the process, we will learn how to create a function which can fit a Gaussian curve for each of the known source's photopeaks. The information from those fits will then be used to find a final line which can help us determine the properties of the unknown source. We will then learn about how to perform a \\(\\chi^2\\) (chi-squared) fit in order to determine how well our predictions for the unknown source fit our data.</p> <p>If you are new to programming, it's recommended that you first take a few minutes to go over this short lesson talking about how to read technical documentation.</p> <p>Let's get started. Click here to continue on to the next section where we will begin setting up our computers for this analysis.</p>"},{"location":"01_setup/","title":"Setup","text":"<p>This lesson will be using Python 3 along with several of its more common libraries: <code>scipy</code>, <code>pandas</code>, <code>matplotlib</code>, and <code>numpy</code>. We will be using Anaconda for our purposes here - Anaconda installs a launcher with Jupyter Notebook as one of the options. Jupyter Notebooks will be used for their cells which make dividing up processes more convenient when working with Python.</p> <p>To get started downloading Anaconda, follow this link. Anaconda is free, and if you don't want to create an account with them click the \"skip registration\" link inside the \"Free Download\" box. Don't download \"Miniconda\", instead, download the full distribution with the Distribution installers for your OS. </p> <p>Next, create a folder to house your project files, named something like \"data-fitting\". After doing this, download and unzip these raw data files. Then, you can move the \"raw-data\" folder into your project folder. </p> <p>Finally, create a new notebook by clicking on <code>New</code> and <code>Notebook</code> and selecting the kernel: <code>Python 3 (ipykernel)</code>. You can then either name your project or keep it untitled.</p> <p>Here's a short video showing the process of opening a Jupyter Notebook and running a code cell (skipping the step of creating a new folder):</p> <p>If you're having trouble loading this video, you may have to open it in a new tab, window, or browser. Click here for the link to the video.</p> <p>Now that we're all setup, click here to continue to the next section where we can begin to learn more about the experiment and how it functions before getting started fitting our data.</p>"},{"location":"02_experiment_details/","title":"The Experiment","text":"<p>In this experiment, gamma spectroscopy data was collected from a lab assignment performed at UCDenver. The lab involved taking data using a spectrometer which was fed from a scintillator and a photomultiplier tube (PMT) to record the energy levels emitted by various radioactive sources. Let's go over the types of radioactive decay and describe what the related instruments are doing so that you can have a better understanding of the data that we're looking at when we start plotting it.</p>"},{"location":"02_experiment_details/#radioactive-decay","title":"Radioactive Decay","text":"<p>There are three main types of radioactive decay associated with emissions, alpha (\\(\\alpha\\)), beta (\\(\\beta\\)), and gamma (\\(\\gamma\\)), named in increasing order of their ionizing properties (\\(\\alpha &gt; \\beta &gt; \\gamma\\)). These three types of emissions are notable also for their ability to be separated from each other using an electromagnetic field - meaning they all have different types of charge. </p> <p>Starting from the element bismuth (Bi-83), every isotope of every element is at least slightly radioactive. In other words, no element has any stable isotopes after bismuth. From bismuth up, the strong force which holds the nucleus together struggles to reach across the distance of the nucleus, meaning the electric force which wants the positively charged protons to repel each other is able to produce more force than is required to hold the nucleus together. For elements lighter than bismuth, every element has at least one stable isotope, but there are still many radioactive isotopes such as Carbon-14 or Potassium-40.</p>"},{"location":"02_experiment_details/#alpha-alpha-decay","title":"Alpha (\\(\\alpha\\)) Decay","text":"<p>This type of decay releases a positively charged alpha particle (sometimes represented as \\(^4_2\\alpha\\)), consisting of two protons and two neutrons (a helium nucleus) being ejected from the atom, reducing its atomic number (Z) by two. It is also the least penetrating of these forms of radiation and can be stopped by just a few centimeters of air, but it has the power to be the most ionizing and destructive of the types we'll cover here. </p> <p>Because of the low penetrating power of alpha particles, they are generally only harmful to life if the radioactive source is swallowed, inhaled, or held closely to the body for prolonged periods of time.</p> <p>NOTE: Don't do that.</p>"},{"location":"02_experiment_details/#beta-beta-decay","title":"Beta (\\(\\beta\\)) Decay","text":"<p>This type of decay releases a beta particle, which can either be a high-speed electron (\\(\\beta^-\\) or beta minus decay), or its antimatter counterpart the positron (\\(\\beta^+\\) or beta plus decay). \\(\\beta^-\\) decay occurs when a neutron decays into a proton, releasing an electron and an antineutrino to conserve charge, while \\(\\beta^+\\) decay occurs when a proton decays into a neutron, releasing a neutrino and positron to conserve charge. So, an atom which undergoes beta decay can gain or lose one atomic number depending on whether it is \\(\\beta^-\\) or \\(\\beta^+\\), but will have an unchanged mass number (A).</p> <p>This type of radiation is in between the other two in terms of both ionization and penetrative power. It is able to pass several millimeters into aluminum, but is generally considered a mild hazard. If exposed to beta radiation for long enough however, it is possible to develop burns similar to those caused by heat. As with alpha radiation, these effects can be exacerbated if the source is swallowed, inhaled, or held for long periods of time.</p>"},{"location":"02_experiment_details/#gamma-gamma-decay","title":"Gamma (\\(\\gamma\\)) Decay","text":"<p>Unlike the other two, gamma decay doesn't release a particle with mass, instead releasing electromagnetic radiation in the form of a chargeless photon of light. Gamma radiation differs from other more familiar forms of light (such as visible light or x-rays), in that it has the shortest wavelengths and thus the highest energies of light (\\(E=\\frac{hc}{\\lambda}\\)). Gamma radiation has many possible sources, but in particle physics it normally occurs after a nucleus undergoes either alpha or beta decay. This decay leaves the nucleus in an excited state, and when the nucleons transition to a lower energy state, it releases one of these high energy photons.</p> <p>This transition is much like the process through which the more common types of light are emitted, where an electron will transition between energy levels, releasing a photon in the process. However, gamma decay transitions involve the strong nuclear force rather than the electromagnetic, producing a photon orders of magnitude more energetic. The typical photon released through electronic relaxation will be in the eV range, usually less than 100eV, while the typical photon released from an atomic nucleus will range from around 1keV to 10MeV of energy.</p> <p>Gamma radiation has the most ability to penetrate materials but is also the least ionizing of these three types of radiation. Natural exposure to gamma radiation is in the range of 1-4 mSv (milli-Seivert) per year, while the lower end of exposure which can begin to cause harmful effects such as cancer are estimated to be around 100 mSv. For reference, your typical chest x-ray will deliver around 5-8 mSv, and a PET/CT scan will deliver between 14-32 mSv. At 1 Sv, the effects of acute radiation sickness can start to be observed, and the dosage at which approximately 50% of those exposed to radiation will die as a result of the exposure (the \\(LD_{50}\\)) is around 3-5 Sv. </p>"},{"location":"02_experiment_details/#instrumentation","title":"Instrumentation","text":"<p>Now that we've gone over the different types of radiation, let's go over how we can detect gamma decay experimentally, and then how we can use these detections to calibrate our instruments and determine the properties of an unknown isotope.</p>"},{"location":"02_experiment_details/#scintillator","title":"Scintillator","text":"<p>The scintillator is the first step in our instrumentation. These devices utilize the properties of certain materials to luminesce when excited by radiation, causing them to re-emit light at a lower and more easily detectable wavelength. For gamma ray detection, photons Compton scatter off of electrons in the structure of the scintillator.</p> <p>Compton scattering describes the effect where high-energy photons interact with loosely-bound valence shell electrons, giving them enough energy to be released from their atoms and ionizing the atom in the process. Those free electrons can then scatter off of other electrons, spreading the energy of the initial gamma ray across multiple electrons. As each of these electrons are recaptured by the ionized atoms, they then release photons at a lower energy.</p> <p>It is by this process that one high energy gamma ray can be turned into several lower energy photons which can then be detected using the second instrument in the experiment.</p>"},{"location":"02_experiment_details/#photomultiplier-tube-pmt","title":"Photomultiplier Tube (PMT)","text":"<p>A PMT is an incredibly sensitive photon detector, and they are normally designed to detect light specifically in the ultraviolet, visible, and near-infrared ranges of the electromagnetic spectrum (which is why we need the scintillator to step-down our high gamma energies).</p> <p>They are typically constructed using an evacuated glass housing with a photocathode on one end, which is engineered to use the photoelectric effect. This again uses energy from light to release electrons, but now in the presence of an electric field. The electric field accelerates the released electrons towards the back of the PMT, and along the way the electrons will strike against arrays of dynodes.</p> <p>Each dynode has a higher potential difference (voltage) than the previous one, and they are designed so that with each electron that strikes them, they release several more. This causes an exponential cascade of electrons to flow through the system until they finally reach the anode at the end of the PMT. Here, the cascade of electrons results in a current which can be detected using a device such as an oscilloscope or a spectrometer, and the results can be recorded. In this experiment, a spectrometer was used to record the data.</p>"},{"location":"02_experiment_details/#spectrometer","title":"Spectrometer","text":"<p>\"Spectrometer\" is a broad class of scientific instruments which can be used for detecting and measuring the spectral components of light. The one used for this experiment was a Universal Computer Spectrometer (UCS). The UCS was the final component in the detection chain and serves as the interface between the analog signal produced by the PMT and the digital data we will analyze. This type of spectrometer is called \"universal\" as it can be configured for a wide range of radiation detection experiments, and \"computer\" as it can be integrated with a digital system capable of binning, processing, and storing pulse data. </p> <p>It works by monitoring the pulses of current generated at the anode of the PMT. Each pulse corresponds to a single detected photon event detected by the scintillator and converted to a current by the PMT. The one used in this experiment contained 2048 channels, with higher channels corresponding to higher detected currents.</p>"},{"location":"02_experiment_details/#data-analysis-and-calibration","title":"Data Analysis and Calibration","text":"<p>The next step in this process is to take the data from the radioactive sources and feed them into a computer using software. As decay events occur in relation to the associated element's half life, it is necessary to run the detection software for several minutes for each element in order to collect as much data as possible. Without running the data for enough time, it may become difficult or impossible to distinguish the signal from the noise. The longer the data is collected, the smaller the margin of error.</p> <p>After an event is measured in a specific channel, that channel's count number is increased by one. This eventually results in noticeable peaks which correspond with the isotope's emission energy. Every isotope has a unique emission energy, allowing the detection of a photon and the calculation of its energy to function like taking the isotope's fingerprint.</p> <p>However, due to the steps of the scintillator and PMT, it is not possible to immediately directly correlate a specific channel with a specific emission energy. It is because of the steps required to be able to detect the gamma rays in the first place that we must also perform significant data analysis to associate channel numbers with emission energies. The resulting relationship between channel numbers and emission energies however should be a linear one. Each channel should correspond to a specific range of gamma energies. The more channels that a spectrometer has, the higher its effective resolution.</p> <p>This system is further limited by the geometry of the setup. The radioactive sources are placed underneath the detector and then radiate equally in every direction, meaning only a fraction will interact optimally with the detector. Some of these emissions will hit the detector with a glancing blow, meaning that they'll have less detected energy than the others, while a small amount of photons actually will go straight into the detector but will then have much of their energy reflected directly out. This distribution of energies will result in a Gaussian curve, where the center of the curve will correspond to the average detected energies of the emitted particles. It will also result in other features which will become visible when plotted which we will go over in the next section, called the Compton continuum and the Compton edge.</p> <p>So, in order to determine what element our unknown radioactive source is, it will be necessary to first perform a curve fit for each one of the peaks detected, and then use the known values for their emissions to perform a second curve fit to relate the channel numbers with their actual energies.</p> <p>This entire process is known as calibration. By using known isotopes, we are testing the parameters of our instruments, allowing us to accurately assign a particle's emission energy levels to a specific range of data channels. Calibration describes the process of establishing a relationship between values using measurements and their uncertainties. This lesson is showing specifically how to calibrate a spectrometer, but you should be thinking about how this process can be generalized and applied to other types of scientific recording.</p> <p>Let's get going, then! Click here to continue to the next section where we'll get started by isolating the photopeaks in our data.</p>"},{"location":"03_isolating_peaks/","title":"Initial Data Plotting","text":"<p>For this section, let's first see the entire process for selecting out the data for a single file so that the method can be applied to all of our data. Just as an example, I'll be demonstrating the process using the <code>Co60.csv</code> file, which you may be able to guess contains a sample of Cobalt-60.</p> <p>Before we can get started plotting our data, we'll want to import the modules which we will use. In the first cell of your Jupyter Notebook, add the import statements:</p> <pre><code>import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy.optimize import curve_fit\nfrom scipy.signal import find_peaks\nfrom scipy.stats import linregress, chi2\n</code></pre> <p>Before plotting, we'll also want to inspect our <code>.csv</code> files to see which rows and columns we're able to use. Here's what the first several lines of the file <code>Co60.csv</code> contain:</p> <p></p> <p>As you can see here, rows 1 through 18 contain data related to the program settings when the data was collected, such as the voltage, fine and coarse grain values, the time and date that the data was collected, and so on. It isn't until row 23 that we start getting actual data points, with row 22 telling us the column names. We can also see that only the first and third columns (Channel &amp; Counts) contain data, with the second column (Energy) being empty. These facts hold true across all of our files, so we can skip the same initial rows and only use the same columns across all of our data.</p> <p>For much of our data, it will also be easier to find our peaks if we set the y-axis to a logarithmic scale instead of a linear one. Let's get started by creating a method which will load and plot our data when given a file path, and which will also have an option to allow us to control how the y-axis is scaled:</p> <pre><code>def plot_data(isotope, y_log=True):\n    data = pd.read_csv(f'raw-data/{isotope}.csv', skiprows=21, usecols=[0,2])\n    # Assign axes and names\n    x_axis = data.iloc[:,0]\n    x_name = data.columns[0]\n\n    y_axis = data.iloc[:,1]\n    y_name = data.columns[1]\n\n    # Create plot\n    plt.scatter(x_axis, y_axis, s=1)\n    plt.xlabel(x_name)\n    plt.ylabel(y_name)\n    plt.title(f\"{isotope} raw data\")\n    plt.savefig(f\"{isotope}_raw_data.png\")\n    if y_log:\n        plt.yscale('log')\n    plt.show()\n\n    return data\n</code></pre> <p>This function only requires the name of the file before the file extension in order to create a plot of the recorded data. If <code>y_log</code> isn't specified, it will default to <code>True</code>:</p> <pre><code>plot_data(\"Co60\", y_log=True)\n</code></pre> <p>Output:</p> <p></p>"},{"location":"03_isolating_peaks/#interpreting-the-plot","title":"Interpreting the Plot","text":"<p>In order to properly calibrate our data, it's necessary that we fully understand what the raw data is telling us. In this case, we should know beforehand that Cobalt-60 has two peak emission spectra: one at 1.175 MeV, and another at 1.333 MeV. However, in this plot, there appear to be at least 3 separate peaks: 1 on the far left of the plot, and 2 on the far right of the plot. As well as those more obvious peaks, we also have 2 bumps which could possibly be considered minor peaks between them. So, how do we know which ones to consider when we begin isolating our peaks? In this case, we have to know what shapes we should expect from gamma spectroscopy data.</p>"},{"location":"03_isolating_peaks/#compton-scattering","title":"Compton Scattering","text":"<p>In gamma spectroscopy, not every emitted photon deposits its full energy directly into the detector. As mentioned previously, the scintillator steps down the photon energies in a process known as Compton scattering, where a photon transfers its energy to a series of electrons. In this process, it is possible for only a fraction of the photon's energy to be absorbed by the electron it's incident upon, with the total deposited energy being reliant on the scattering angle. Between the leftmost and rightmost peaks in the plot, you can see a gently sloping distribution with the extrema defined by the two bumps/minor peaks. This is known as the Compton plateau or Compton continuum.</p> <p>The upper limit of partial energy deposition forms what's called the Compton edge (or Compton cliff): a relatively sharp drop at the high-energy end of the Compton continuum. In the plot for Cobalt-60, this can be seen just before the peaks on the right hand side. The energy of this edge corresponds to the maximum energy that a photon can transfer in a single Compton scattering event, which occurs when the photon enters the detector and deposits a significant portion of its energy before being reflected \\(180^\\circ\\). The smaller \"bumps\" before the main peaks are not true photopeaks, but part of the Compton scattering pattern.</p> <p>Meanwhile, the far-left peak is most likely the result of backscatter radiation. Backscatter peaks arise when gamma photons scatter off of nearby objects, such as walls, the table, or the radiation shielding, before they re-enter the detector with significantly reduced energy. These photons can produce a distinct peak at lower channels, to the left of the main Compton continuum. </p> <p>It is also possible for the lower energy peaks to occur as a result of the detector material. When an incoming gamma photon ejects an inner shell electron in the scintillator, it will emit a characteristic X-ray depending on what it's made out of, NaI in the case of this experiment. This is called an X-ray escape peak. </p> <p>This gives our low energy peak two possible explanations: either it is backscatter radiation, or it is an escape peak emitted by our NaI detector. These explanations don't have to be mutually exclusive either - it's possible that both features occur at the same or overlapping energies.</p> <p>It's important to understand that the Compton edge only shows the maximum for partial energy loss, not the maximum possible energy that can be detected. Past the edge, a photon gets closer to being fully absorbed by the scintillator, meaning everything before the furthest right peaks represents events where the photon is at least partially reflected out of the detector, and the centers of the peaks represent where most or all of the photon's energy is absorbed by the scintillator and passed on to the PMT. Every true photopeak should be preceded by a Compton continuum and edge. As our photopeaks are so near each other in our Co-60 data, their respective Compton features overlap.</p> <p>Altogether, this plot shows:</p> <ul> <li> <p>A backscatter reflective peak and/or an X-ray escape peak on the far left,</p> </li> <li> <p>A Compton continuum in the mid range,</p> </li> <li> <p>A Compton edge before the first photopeak,</p> </li> <li> <p>And finally, the two true photopeaks of Cobalt-60 on the far right.</p> </li> </ul> <p>Now that we understand the features of our plot, let's move forward to selecting the data from our peaks.</p>"},{"location":"03_isolating_peaks/#selecting-the-photopeaks","title":"Selecting the Photopeaks","text":"<p>There are two main methods that can be used to find our peaks: </p> <ol> <li> <p>Using the <code>find_peaks</code> module in <code>scipy</code></p> </li> <li> <p>Visually inspecting the data.</p> </li> </ol> <p>Before attempting either of these methods, let's edit our <code>plot_data</code> function to return the <code>data</code> variable so that it can be saved and used elsewhere. At the end of the function, simply add <code>return data</code>. Then, when you call the function, you can set <code>data</code> to a new variable, turning our function call into:</p> <pre><code>Co_60 = plot_data(\"Co60.csv\", y_log=True)\n</code></pre> <p>Now we can use the <code>Co_60</code> variable to find our peaks. Let's look at the process for using <code>find_peaks</code> first.</p>"},{"location":"03_isolating_peaks/#find_peaks","title":"<code>find_peaks</code>","text":"<p>In order to find our peaks with this method, we'll first want to assign  <code>Co_60</code>'s x- and y-axes to variables so that we can plot them against what <code>find_peaks</code> returns as a comparison. The result that we're looking for is to have one selected point at the top of our two right peaks.</p> <p>The <code>find_peaks</code> module can take as optional parameters: <code>height</code>, <code>threshold</code>, <code>distance</code>, <code>prominence</code>, <code>width</code>, <code>wlen</code>, <code>rel_height</code>, and <code>plateu_size</code>. Locating the peaks will require careful manipulation of these values in order to exclude noise and data that isn't the peaks. For more complete descriptions of these parameters, click here to go to the official documentation.</p> <p>Without assigning any of these parameters a value, <code>find_peaks</code> selects indices in the data that are preceded by and followed by smaller values. It returns them along with a dictionary which contains the values it calculates for the optional parameters (heights, thresholds, prominences, etc). If your data is somehow perfectly smooth and noiseless, then no further steps would be required. To show what this looks like when used with our noisy sample, let's plot our data underneath the found peaks without first trying to filter for certain parameters:</p> <pre><code>x_values = Co_60.iloc[:,0]\ny_values = Co_60.iloc[:,1]\n# Find peaks\npeaks, _ = find_peaks(y_values)\n# Plot data\nplt.scatter(x_values, y_values, s=1)\n# Plot peaks\nplt.plot(peaks, y_values[peaks], 'x', color='red')\nplt.show()\n</code></pre> <p>Output:</p> <p></p> <p>Each one of the plotted 'x's here is being counted as another individual 'peak'. So, to narrow down our choices, let's start trying to estimate the parameters. Let's try adjusting <code>distance</code>, <code>prominence</code>, and <code>width</code>. The distance argument sets the minimum distance that should exist between two peaks. The <code>prominence</code> argument measures the vertical distance between the top of the peak and the lowest contour line that doesn't enclose a higher peak (as opposed to <code>height</code> which measures the distance from the top of the peak to y=0). The width argument measures the horizontal span of a single peak. </p> <p>You are encouraged to attempt to solve for some appropriate values for these parameters here.</p> Show solution <p>  If we provide an estimate for these values at `150`, `100`, and `60` respectively, we'll be able to select out the two peaks we want:   <pre><code>x_values = Co_60.iloc[:,0]\ny_values = Co_60.iloc[:,1]\npeaks, _ = find_peaks(y_values, distance=150, prominence=100, width=60)\nplt.plot(x_values, y_values)\nplt.plot(peaks, y_values[peaks], 'x', color='red')\nplt.show()\n</code></pre> </p> <p>After finding the appropriate parameters, your results should look similar to the following:</p> <p></p> <p>Now, we'll have to save the data around the peaks. This can be done with a <code>for loop</code> where we select a number of points on either side of the peak, extract that portion of the dataset, and store it for later use. We'll also include safeguards to ensure we don't go outside of the bounds of the data:</p> <pre><code>peak_ranges = []\nfor peak in peaks:\n    left=max(0, peak-60)\n    right=min(len(y_values)-1, peak + 60)\n    peak_ranges.append(Co_60.iloc[left:right].copy())\n</code></pre> <p>Each entry in <code>peak_ranges</code> now holds a slice of copied data centered around one of the detected peaks. We can plot each of these slices to visually confirm that the correct regions were captured:</p> <pre><code>for peak in peak_ranges:\n    plt.scatter(peak.iloc[:,0], peak.iloc[:,1], s=1)\nplt.title('Isolated peaks')\nplt.xlabel('Channels')\nplt.ylabel('Counts')\nplt.show()\n</code></pre> <p>Output:</p> <p></p> <p>This also highlights one of the shortcomings of the <code>find_peaks</code> function. As we can see with the peak on the left, our selection won't necessarily be centered. It found the highest point in the left peak, which happens to be an outlier in the data, and assumes that this point is the center of the peak.</p> <p>You could then set each peak to a new variable so that it can be used in the next section by using:</p> <pre><code>Co_60P1 = peak_ranges[0]\nCo_60P2 = peak_ranges[1]\n</code></pre> <p>While <code>find_peaks</code> is generally very powerful, it requires careful tuning of multiple parameters. It is also possible to run into plots where the peaks that you see can't easily be distinguished from noise using this module, or where it is impossible to set the parameters so as to include all peaks, such as when two peaks overlap with each other while others are distinct. There are work arounds for these issues, such as using multiple instances of <code>find_peaks</code> to select peaks with different parameters in the same data, but it may be better to use another method for isolating your peaks depending on which problems you're running into.</p>"},{"location":"03_isolating_peaks/#visual-inspection","title":"Visual inspection","text":"<p>While it may not be programmatically optimized, one of the easier ways to isolate peaks is by eye: simply estimate where the boundaries of the peaks are and use those numbers to splice the plot. If it's off - adjust your numbers until the peaks are as close to centered as you can get them. If you plot your estimates as you go, you will also be contracting what's shown of the x-axis, providing you with new tick marks with less distance between them, allowing you to splice again more accurately.</p> <p>Let's try:</p> <pre><code>Co_60P1 = Co_60.iloc[1600:1900].copy()\nplt.scatter(co_peak_one.columns[0], co_peak_one.columns[1], s=1)\nplt.show()\n</code></pre> <p>Output:</p> <p></p> <p>This isn't perfect, but makes it easier to find the appropriate limits. To center the peak, let's use the data range [1680:1815]:</p> <p></p> <p>Nice! Now we can use the same process to isolate our second peak, giving us:</p> <pre><code>Co_60P2 = Co_60.iloc[1850:2025].copy()\nplt.scatter(Co_60P2.iloc[:,0], Co_60P2.iloc[:,1], s=1)\nplt.show()\n</code></pre> <p>Output:</p> <p></p> <p>Now that we have our first peaks isolated, click here to continue on to the next section where we'll learn how to fit our data to an equation.</p>"},{"location":"04_fitting_curves/","title":"<code>curve_fit</code>","text":"<p>The parameters that <code>curve_fit</code> works from are given by the official documentation as: <code>f</code>, <code>xdata</code>, <code>ydata</code>, <code>p0</code>, <code>sigma</code>, <code>absolute_sigma</code>, <code>check_finite</code>, <code>bounds</code>, <code>method</code>, <code>jac</code>, <code>full_output</code>, <code>nan_policy</code>, and <code>**kwargs</code>. Only the first three parameters are required: <code>f</code>, <code>xdata</code>, and <code>ydata</code>, but including the fourth, <code>p0</code>, is often essential for guiding the fit, especially when the function is nonlinear or has multiple parameters. <code>p0</code> defaults to all ones if not specified, but this often fails for more complex models. </p> <p>Only these four will be covered here, but feel free to explore the documentation provided if you wish to use everything that <code>curve_fit</code> has to offer.</p> <p>In <code>curve_fit</code>, <code>f</code> is our model function. This requires the creation of another function which will be called in <code>curve_fit</code>. It is necessary that the first argument that the function <code>f</code> takes is the independent variable, which is the x-axis in the case of our data. The other parameters of this function will be the remaining unknown variables of the equation.</p> <p><code>xdata</code>, and <code>ydata</code> are exactly what they sound like: the x and y data that you're fitting to the model function.</p> <p><code>p0</code> is the initial guess for the unknown variables/parameters of the model function. It is given as an array where every guess is listed in the order that their associated parameters are listed in the function definition (i.e. if the function <code>f</code> is: <code>def function(x, a, b)</code>, <code>p0</code> would include <code>[a_estimate, b_estimate]</code>). </p> <p>Let's get started on creating our model function.</p>"},{"location":"04_fitting_curves/#the-gaussian","title":"The Gaussian","text":"<p>The Gaussian (or Normal) Distribution is a common equation that you'll run into in statistical mechanics. It describes a symmetric, bell-shaped curve centered at a mean value, \\(\\mu\\) (mu), with its width determined by the standard deviation, \\(\\sigma\\) (sigma), and its offset above the x-axis given by D (called the y-offset). It has the form:</p> \\[f(x)=\\frac{1}{\\sqrt{2\\pi \\sigma^2}}e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}+D\\] <p>In gamma spectroscopy, we are using the Gaussian as a model for how our detected energy counts are spread around the central photopeak. Variations in the emission and detection of our photons have caused the recorded values to \"smear\" into a normal distribution, with the most common value being the center of the curve. Fitting our data to this curve then allows us to extract the value for the central energy, \\(\\mu\\), as well as its margin of error/standard deviation, \\(\\sigma\\).</p> <p>In our equation, the first term (\\(\\frac{1}{\\sqrt{2\\pi \\sigma^2}}\\)) is called the normalization constant. Its purpose is to set the value of the integral of a Gaussian curve to be equal to 1, meaning 100% of the data fits under the curve, but this term can actually be dropped for our purposes. We can thus set our equation to be equal to:</p> \\[f(x)=A \\cdot e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}+D\\] <p>In this equation:</p> <ul> <li> <p>A controls the height/amplitude of the peak</p> </li> <li> <p>\\(\\mu\\) is the mean value in the center (the energy of the photon)</p> </li> <li> <p>\\(\\sigma\\) relates to the standard deviation of the line</p> </li> <li> <p>D is the y-offset.</p> </li> </ul> <p>\\(\\sigma\\) is one of the more difficult of these variables to estimate unless you are already at least somewhat familiar with the Gaussian distribution. One \\(\\sigma\\) can be estimated as a distance from the central \\(\\mu\\) value which includes just over one third of the total area under the curve. Keeping the following visual in mind can be very helpful for trying to estimate \\(\\sigma\\):</p> <p></p> <p>[Image from https://tikz.net/gaussians/]</p> <p>Let's create a <code>gaussian</code> function which simply returns our equation \\(f(x)\\):</p> <pre><code>def gaussian(x, amplitude, mu, sigma, y_offset):\n    return amplitude * np.exp(-((x-mu)**2) / (2 * sigma**2)) + y_offset\n</code></pre> <p>Additionally, our data will include some peaks which are so close to each other that they actually overlap. Overlapping peaks are common when two gamma emissions have close energies. This can be modelled using the equation:</p> \\[f(x)=A_1 \\cdot e^{-\\frac{(x-\\mu_1)^2}{2\\sigma^2_1}}+A_2 \\cdot e^{-\\frac{(x-\\mu_2)^2}{2\\sigma^2_2}}+...+A_n \\cdot e^{-\\frac{(x-\\mu_n)^2}{2\\sigma^2_n}}+D\\] <p>Where <code>n</code> is given by the number of overlapping peaks. Luckily, our data shouldn't include more than two overlapping peaks, so we only need to create a function to find the parameters of a double gaussian:</p> <pre><code>def double_gaussian(x, amp_1, mu_1, sigma_1, amp_2, mu_2, sigma_2, y_offset):\n    return (\n        amp_1 * np.exp(-((x-mu_1)**2) / (2 * sigma_1)**2) \n        + (amp_2 * np.exp(-((x-mu_2)**2) / (2 * sigma_2)**2)) \n        + y_offset\n    )\n</code></pre> <p>For reference (if you're interested in attempting to find all of the peak fits before continuing to the next section), here is a table with the provided isotopes and the known emission energies of their most prominent photopeaks:</p> Isotope Energy (keV) Na-22 511, 1274.54 Mn-54 834.84 Co-57 122.06 Co-60 1173.23, 1332.49 Cd-109 88.03 Ba-133 81, 276.4, 302.85, 356.01, 383.85"},{"location":"04_fitting_curves/#using-curve_fit","title":"Using <code>curve_fit</code>","text":"<p>Now we can finally use <code>curve_fit</code> to start extracting values from our data. <code>curve_fit</code> can return five different values, <code>popt</code>, <code>pcov</code>, <code>infodict</code>, <code>mesg</code>, and <code>ier</code>, but only the first two are valuable for our purposes. <code>popt</code> is a one dimensional array containing estimates for the optimal parameter values, while <code>pcov</code> contains a 2-D array with estimates for the covariance of <code>popt</code>. For the values of the optimal parameter's uncertainties, only the square roots of the diagonal values of <code>pcov</code> are needed - each entry shows the relationship between each variable with the diagonal values being the relationships of each variable with itself.</p> <p>Let's create a function for each of our gaussian functions which will return the relevant values of <code>popt</code> and <code>pcov</code>. The only difference between these functions will be which equation they call as the model functions:</p> <pre><code>def find_gaussian_values(xdata, ydata, p0):\n    return curve_fit(gaussian, xdata, ydata, p0=p0)\n\ndef find_dgaussian_values(xdata, ydata, p0):\n    return curve_fit(double_gaussian, xdata, ydata, p0=p0)\n</code></pre> <p>Next, let's create a function which will take our <code>popt</code> and <code>pcov</code> arrays and use them to create a plot of our data against the line of best fit. It will be able to take an element name and our emission value in order to title each plot. We can also make it capable of plotting either a gaussian or a double gaussian with a simple <code>True</code>/<code>False</code> flag, and then use an <code>if</code>/<code>else</code> tree to differentiate between the two. It will also print out the found parameters and their uncertainties, as well as return our \\(\\mu\\) values and their uncertainties so they can be assigned to variables and used later:</p> <pre><code>def plot_best_fit(element, emission_peak, xdata, ydata, popt, pcov, d_gaussian=False):\n    uncertainties = np.sqrt(np.diag(pcov))\n    # Create labels for printing results\n    if not d_gaussian:\n        labels = ['Amplitude', 'Mean', 'Sigma', 'Y-offset']\n    else:\n        labels = ['Amplitude 1', 'Mean 1', 'Sigma 1', \n                  'Amplitude 2', 'Mean 2', 'Sigma 2',\n                  'Y-offset']\n    if not d_gaussian:\n        mu = popt[1]\n        mu_uncertainty = uncertainties[1]\n    else:\n        mu_1, mu_2 = popt[1], popt[4]\n        mu_1_uncertainty, mu_2_uncertainty = uncertainties[1], uncertainties[4]\n\n    # Plot raw data of peak\n    plt.scatter(xdata, ydata, label='Data', marker='.', color='blue')\n\n    # Plot line of best fit and draw a vertical line through mu value\n    if not d_gaussian:\n        plt.plot(xdata, gaussian(xdata, *popt), label='Best fit', color='red')\n        plt.axvline(mu, color='green', linestyle='-.', label=f'$\\mu: {mu:.2f}\\pm{mu_uncertainty:.2f}$')\n    else:\n        plt.plot(xdata, double_gaussian(xdata, *popt), label='Best fit', color='red')\n        plt.axvline(mu_1, color='green', linestyle='-.', label=f'$\\mu_1: {mu_1:.2f}\\pm{mu_1_uncertainty:.2f}$')\n        plt.axvline(mu_2, color='black', linestyle='-.', label=f'$\\mu_2: {mu_2:.2f}\\pm{mu_2_uncertainty:.2f}$')\n\n    # Title plot, show legend, and label axes\n    plt.title(f'{element}: {emission_peak} Peak')\n    plt.xlabel('Channel')\n    plt.ylabel('Counts')\n    plt.legend()\n    plt.show()\n\n    # Print and return results\n    print('Best fit parameters:')\n    for name, val, err in zip(labels, popt, uncertainties):\n        print(f'{name}: {val:.2f} +/- {err:.2f}')\n\n    if not d_gaussian:\n        return mu, mu_uncertainty\n    else:\n        return mu_1, mu_1_uncertainty, mu_2, mu_2_uncertainty\n</code></pre> <p>With these functions, we can now run our first <code>curve_fit</code> after estimating the amplitude, center channel of the peak, the uncertainty, and the y-offset. Let's take a look again at our first isolated curve:</p> <p></p> <p>To estimate the amplitude, we can subtract a rough estimate of where the bottom of the curve lies on the y-axis from where the top of the curve lies. To estimate the mean channel number, we can simply eyeball where the center of the curve lies. To estimate the standard deviation, we can recall the image from before, and to estimate the y-offset we can view where the bottom of the curve lies on the y-axis once again. Let's go with: <code>[175, 1750, 20, 50]</code> for these values.</p> <pre><code>p0=[175, 1750, 20, 50]\n\nCo_60P1_popt, Co_60P1_pcov = find_gaussian_values(\n    co_peak_one.iloc[:,0].values, \n    co_peak_one.iloc[:,1].values, \n    p0\n    )\n\nCo_60P1_mu, Co_60P1_mu_uncert = plot_best_fit(\n    'Co-60', '1.175 MeV',\n    co_peak_one.iloc[:,0].values, co_peak_one.iloc[:,1].values, \n    Co_60P1_popt, Co_60P1_pcov\n    )\n</code></pre> <p>Output:</p> <p></p> <pre><code>Best fit parameters:\nAmplitude: 160.75 +/- 8.62\nMean: 1747.79 +/- 0.57\nSigma: 33.09 +/- 2.04\nY-offset: 54.67 +/- 9.43\n</code></pre> <p>And we can repeat our steps to estimate the values of our second peak:</p> <pre><code>p0=[150, 1940, 30, 25]\n\nCo_60P2_popt, Co_60P2_pcov = find_gaussian_values(\n    co_peak_two.iloc[:,0].values, \n    co_peak_two.iloc[:,1].values, \n    p0\n    )\n\nCo_60P2_mu, Co_60P2_mu_uncert = plot_best_fit(\n    'Co-60', '1.333 MeV',\n    co_peak_two.iloc[:,0].values, co_peak_two.iloc[:,1].values, \n    Co_60P2_popt, Co_60P2_pcov\n    )\n</code></pre> <p>Output:</p> <p></p> <pre><code>Best fit parameters:\nAmplitude: 156.96 +/- 3.83\nMean: 1939.07 +/- 0.43\nSigma: 39.02 +/- 1.22\nY-offset: 9.62 +/- 4.18\n</code></pre> <p>Awesome! You've completed your first set of <code>curve_fit</code>s and have found the numbers which we can later associate with the radiation energies. This process has also resulted in us setting the variables <code>Co_60P1_mu</code>, <code>Co_60P1_mu_uncert</code>, <code>Co_60P2_mu</code>, and <code>Co_60P2_mu_uncert</code>. </p> <p>In order to make our work easier later, let's create a new cell at the bottom of our notebook which we'll use to keep a dictionary of all of our elements, their calculated channel numbers and uncertainties, and their associated emission energies (in keV):</p> <pre><code>value_bank = {\n    ('Co-60', Co_60P1_mu, Co_60P1_mu_uncert, 1175),\n    ('Co-60', Co_60P2_mu, Co_60P2_mu_uncert, 1333),\n}\n</code></pre> <p>This is where you'll store variables for each of the remaining files emission peaks, which will be used in the next section. </p> <p>After we have performed this process for the remaining files, we'll have successfully extracted the mean channel numbers (\\(\\mu\\)) for every detected emission peak. From there, we can begin the process of fitting our data to a straight line. Click here to continue to the next section where we will look at how to plot all of our data as well as label it and include our margins of error.</p>"},{"location":"05_linregress/","title":"Plotting the Known Data","text":"<p>As mentioned previously, the relationship between the mean channel numbers that we find and the actual emission energy levels should be a linear one. This means that we'll be using the equation of a line:</p> \\[y=mx+b\\] <p>We can now plot our data with the found channel numbers, C, along the y-axis, and the known emission energies, E, along the x-axis, giving us:</p> \\[C=mE+b\\] <p>We should be able to fit all of our data to this line. After doing this, we can simply plug the mean channel numbers for the peaks of our unknown source into this line to find what the emission energies of the source are, along with an estimate of their uncertainties.</p> <p>When we're done finding the peaks of all of our data, our <code>value_bank</code> should look like:</p> <pre><code>value_bank = {\n    ('Na-22', Na_22P1_mu, Na_22P1_mu_uncert, 511),\n    ('Na-22', Na_22P2_mu, Na_22P2_mu_uncert, 1274.54),\n    ('Mn-54', Mn_54P1_mu, Mn_54P1_mu_uncert, 834.84),\n    ('Co-57', Co_57P1_mu, Co_57P1_mu_uncert, 122.06),\n    ('Co-60', Co_60P1_mu, Co_60P1_mu_uncert, 1173.23),\n    ('Co-60', Co_60P2_mu, Co_60P2_mu_uncert, 1332.49),\n    ('Cd-109', Cd_109P1_mu, Cd_109P1_mu_uncert, 88.03),\n    ('', Ba_133P1_mu, Ba_133P1_mu_uncert, 81),\n    ('Ba-133', Ba_133P2_mu, Ba_133P2_mu_uncert, 276.4),\n    ('', Ba_133P3_mu, Ba_133P3_mu_uncert, 302.85),\n    ('Ba-133', Ba_133P4_mu, Ba_133P4_mu_uncert, 356.01),\n    ('', Ba_133P5_mu, Ba_133P5_mu_uncert, 383.85),\n} \n</code></pre> <p>NOTE: Only two of the Ba-133 peaks are labelled. This is because they are so close to themselves and other isotopes that their labels would overlap if kept.</p> <p>In order to handle the known peaks separately from our unknown ones, we've also created a separate bank for the peaks from the unknown source:</p> <pre><code>unknown_values = {\n    ('Unknown Peak 1', unknown_P1_mu, unknown_P1_mu_uncert),\n    ('Unknown Peak 2', unknown_P2_mu, unknown_P2_mu_uncert)\n}\n</code></pre> <p>To plot our known peaks, we'll create a <code>for loop</code> to go through <code>value_bank</code> and separate everything into their own arrays. We'll first initialize our arrays and then append the values into them with our loop. As the uncertainties are pretty small, we'll also want to amplify them so that they'll be visible when plotted. This should be fine as long as we note this exaggeration somewhere in our plot.</p> <pre><code>uncertainty_scale = 50\nlabels             = []\ntested_means       = []\ntested_uncertainty = []\nknown_peaks        = []\nfor (label, tested, uncertainty, known) in value_bank:\n    labels.append(label)\n    tested_means.append(tested)\n    tested_uncertainty.append(uncertainty * uncertainty_scale)\n    known_peaks.append(known)\n</code></pre> <p>With <code>known_peaks</code> and <code>tested_means</code> now separated, we can use <code>scipy</code>'s <code>linregress</code> to calculate the slope and intercept of our line, as well as their respective margins of error:</p> <pre><code>result        = linregress(known_peaks, tested_means)\nslope         = result.slope\nintercept     = result.intercept\nstderr        = result.stderr\nintercept_err = result.intercept_stderr\n</code></pre> <p>We can now plot our data so far to make sure everything is going according to plan. If you followed along with this lesson and found the peak values separately, here is when it will be possible to see if you made an error while selecting peaks that you can go back and correct. </p> <p>For plotting the line of best fit, we'll first create a line which uses the calculated slope and intercept values along with <code>np.linspace</code>. We'll add some space on either side of our max and min values to give our data points some visual breathing room. Then we can plot our error bars, display our labels for each data point, and plot our data, the line of best fit, and label our axes:</p> <pre><code># Create the line of best fit\nx_line = np.linspace(min(known_peaks)-100, max(known_peaks)+100, 100)\ny_line = slope*x_line + intercept\n\n# Plot error bars\nplt.errorbar(\n    known_peaks, tested_means,\n    yerr=tested_uncertainty,\n    fmt='o', markersize=2, color='black',\n    ecolor='red', elinewidth=1, capsize=2,\n    label='Known Emissions'\n)\n\n# Show labels next to their respective data points\nfor x, y, label in zip(known_peaks, tested_means, labels):\n    plt.text(x+50, y, label, fontsize=9, ha='left', va='top')\n\n# Plot best fit line and data points\nplt.scatter(known_peaks, tested_means, s=1)\nplt.plot(x_line, y_line, label=f'$C=({slope:.2f}\\pm{stderr:.2f})\\cdot E+({intercept:.2f}\\pm {intercept_err:.2f})$')\nplt.xlabel('Emission Energy (keV)')\nplt.ylabel('Measured Channel')\nplt.title('Line of Best Fit')\nplt.legend(title=f'Uncertainties exaggerated {uncertainty_scale}x', fontsize='small')\nplt.show()\n</code></pre> <p>Output:</p> <p></p>"},{"location":"05_linregress/#plotting-the-unknown-isotope","title":"Plotting the Unknown Isotope","text":"<p>In order to find the emission energies for our unknown isotope, we'll have to rearrange our linear equation to solve for E. As \\(C=mE+b\\):</p> \\[E = \\frac{C-b}{m}\\] <p>Now that we have our slope and intercept from the results of <code>linregress</code>, we can start finding our unknown energy values as well as their uncertainties. We're looking for the uncertainties in the unknown energy as it is possible that when we search gamma tables to try and find our isotope we may get multiple energy results near our central peak. To find the energy uncertainties, we'll have to use error propagation. </p>"},{"location":"05_linregress/#error-propagation","title":"Error Propagation","text":"<p>For a given function:</p> \\[f(x_1, x_2,...,x_n)\\] <p>Where each variable \\(x_i\\) has an uncertainty \\(\\sigma_{x_i}\\), the uncertainty in f, \\(\\sigma_f\\), can be found as:</p> \\[\\sigma_f = \\sqrt{     (\\frac{\\delta f}{\\delta x_1}\\sigma_{x_1}^2) +      (\\frac{\\delta f}{\\delta x_2}\\sigma_{x_2}^2) + ... +      (\\frac{\\delta f}{\\delta x_n}\\sigma_{x_n}^2) }\\] <p>As we're looking for \\(\\sigma_E\\), this means our equation will look like:</p> \\[\\sigma_E = \\sqrt{     (\\frac{\\delta E}{\\delta C}\\sigma_C)^2 +     (\\frac{\\delta E}{\\delta b}\\sigma_b)^2 +     (\\frac{\\delta E}{\\delta m}\\sigma_m)^2 }\\] \\[\\sigma_E = \\sqrt{     (\\frac{1}{m}\\sigma_C)^2 +      (-\\frac{1}{m}\\sigma_b)^2 +     (-\\frac{C-b}{m^2}\\sigma_m)^2 }\\] <p>We can now use all of this information to include our unknown isotopes in the plot as well as return the possible emission energy ranges. First, we'll want to create our new arrays for the labels, the means and uncertainties, and their associated energy values. We can also print out our energy values so that we can later compare the ranges to gamma tables:</p> <pre><code>unknown_labels        = []\nunknown_means         = []\nunknown_uncertainties = []\npredicted_energies    = []\npredicted_uncertainty = []\n\nunknown_values = [\n    ('Unknown Peak 1', unknown_P1_mu, unknown_P1_mu_uncert),\n    ('Unknown Peak 2', unknown_P2_mu, unknown_P2_mu_uncert)\n]\nfor label, tested, uncertainty in unknown_values:\n    unknown_labels.append(label)\n    unknown_means.append(tested)\n    unknown_uncertainties.append(uncertainty)\n\n    # Convert to energy\n    energy = (tested - intercept) /slope\n    energy_uncertainty = np.sqrt(\n        ((uncertainty/slope)**2) +\n        ((-intercept_err/slope)**2) +\n        ((-(tested-intercept)*stderr/(slope**2))**2)\n    )\n    predicted_energies.append(energy)\n    predicted_uncertainty.append(energy_uncertainty)\n    print(f'{label}: {energy:.2f} keV +/- {energy_uncertainty:.2f} keV')\n    print(f'({energy-energy_uncertainty:.2f} keV to {energy+energy_uncertainty:.2f} keV)')\n</code></pre> <p>Output:</p> <pre><code>Unknown Peak 1: 690.28 keV +/- 15.45 keV\n(674.83 keV to 705.73 keV)\nUnknown Peak 2: 1097.36 keV +/- 20.35 keV\n(1077.00 keV to 1117.71 keV)\n</code></pre> <p>As we needed the non-exaggerated uncertainties to accurately calculate the energy ranges, we'll exaggerate them elsewhere and use that new array for plotting. Let's go ahead and plot our error bars and add these data points to our plot:</p> <pre><code>exagg_unk = [u * uncertainty_scale for u in unknown_uncertainties]\n\nplt.errorbar(\n    predicted_energies, unknown_means,\n    yerr=exagg_unk,\n    fmt='o', markersize=2, color='grey',\n    ecolor='green', elinewidth=1, capsize=2,\n    label='Unk. Emissions'\n)\nfor x, y, label in zip(known_peaks, tested_means, labels):\n    plt.text(x+50, y, label, fontsize=9, ha='left', va='top')\nfor x, y, label in zip(predicted_energies, unknown_means, unknown_labels):\n    plt.text(x+50, y, label, fontsize=9, ha='left', va='top')\nplt.scatter(known_peaks, tested_means, s=1)\nplt.scatter(predicted_energies, unknown_means, s=1)\nplt.plot(x_line, y_line, label=f'$C=({slope:.2f}\\pm{stderr:.2f})\\cdot E+({intercept:.2f}\\pm {intercept_err:.2f})$')\nplt.xlabel('Emission Energy (keV)')\nplt.ylabel('Measured Channel')\nplt.title('Line of Best Fit')\nplt.legend(title=f'Uncertainties exaggerated {uncertainty_scale}x', fontsize='small')\nplt.show()\n</code></pre> <p>Output:</p> <p></p>"},{"location":"05_linregress/#finding-matching-isotopes","title":"Finding Matching Isotopes","text":"<p>An important thing to note when trying to find which isotopes our unknown source could potentially be, is that we've never specified that the source contains only one isotope. It's entirely possible that it has two isotopes that have our observed peaks instead. </p> <p>In searching for our source, we'll be using the website https://atom.kaeri.re.kr/old/gamrays.html. Here, we can enter our energy range as well as a half-life, and receive an output of multiple isotopes. As the sources had not been changed in multiple years at UCDenver, we can estimate a half-life of at least two months.</p> <p>Using this tool, our found energy ranges, and an estimated half-life minimum of 60 days, the matches we find for our first peak are:</p> Nuclide Energy (keV) Half-Life Pm-144 696.49 363 D Nb-94 702.622 20400 Y <p>And for our second peak, we get the matches:</p> Nuclide Energy (keV) Half-Life Sn-123 1088.64 129.2 D Te-121 1102.15 164.2 D Zn-65 1115.55 243.93 D <p>As there are no overlapping elements between our first and second peaks, this means that we will have to compare all of the peak one options against peak two options in order to determine which combination of isotopes our source most likely contained.</p> <p>In order to perform this final analysis, we'll need to perform something known as a \\(\\chi^2\\) (or chi-squared) fit. Click here to continue on to our next section where we will learn how to use such fits to assess the quality of our findings.</p>"},{"location":"06_chi_squared/","title":"What is a \\(\\chi^2\\) Fit?","text":"<p>A \\(\\chi^2\\) (or chi-squared/weighted least squares fit) is a way of fitting a model to data when each point has a different uncertainty. </p> <p>If your data points are \\((O_i, C_i)\\), (where O is your Observation data, and C is your Calculated or expected data) and each \\(O_i\\) has an uncertainty \\(\\sigma_i\\), then the best-fit line minimizes the quantity:</p> \\[\\chi^2 = \\sum_{i}(\\frac{O_i - C_i}{\\sigma_i})^2\\]"},{"location":"06_chi_squared/#why-use-a-chi2-test","title":"Why Use a \\(\\chi^2\\) Test?","text":"<p>Now that we've predicted the emission energies of the unknown peaks and identified possible isotopic matches using gamma emission tables, we need a way to quantify how well each candidate matches our observations. This is where the \\(\\chi^2\\) test becomes valuable. We are able to run the test by setting \\(O_i\\) as the measured energy from the sample, \\(C_i\\) as the known emission energy from the candidates, and \\(\\sigma_i\\) as the uncertainty in \\(O_i\\). This means that we are performing a weighted fit: the smaller an observations uncertainty, the more it will matter for determining the quality of the fit.</p> <p>For each combination of candidate isotopes, we'll compute the total chi-squared value. Once we calculate \\(\\chi^2\\), we can then also calculate the reduced chi-squared statistic. This lets us move beyond eyeballing the tables and actually rank our findings based on how statistically compatible they are with the observed data.:</p> \\[\\chi^2_{red}=\\frac{\\chi^2}{\\nu}=\\frac{\\chi^2}{N-p}\\] <p>Where:</p> <ul> <li> <p>\\(N\\) is the number of data points</p> </li> <li> <p>\\(p\\) is the number of fitted parameters</p> </li> <li> <p>\\(\\nu\\) is the degrees of freedom</p> </li> </ul> <p>We'll perform a \\(\\chi^2\\) calculation for each possible pairing between a peak-1 candidate and a peak-2 candidate. The closer \\(\\chi^2_{red}\\) is to 1, the better the match. Large values indicate a poor match, while very small values might suggest that our uncertainties are too large or that the candidate is suspiciously good. For each of our possible 6 pairs, we'll compare their known emission energies to our observed values. So, for each pair we will compute:</p> \\[\\chi^2 = (\\frac{E_{obs,1}-E_{cand,1}}{\\sigma_1})^2 + (\\frac{E_{obs,2}-E_{cand,2}}{\\sigma_2})^2\\] <p>Because we're not fitting any parameters, only testing two fixed candidate emission energies, the degrees of freedom, \\(\\nu\\), is simply 2, giving us:</p> \\[\\chi^2_{red}=\\frac{\\chi^2}{2}\\]"},{"location":"06_chi_squared/#implementing-the-chi2-calculation","title":"Implementing the \\(\\chi^2\\) calculation","text":"<p>The first thing that we'll want to do is place our candidate isotopes into arrays so that we can later use nested <code>for loops</code> to compare them all against each other:</p> <pre><code>peak_one = [\n    ('Pm-144', 696.490),\n    ('Nb-94', 702.622)\n]\n\npeak_two = [\n    ('Sn-123', 1088.64),\n    ('Te-121', 1102.15),\n    ('Zn-65', 1115.55)\n]\n</code></pre> <p>We'll also want to place our predicted energies and their respective uncertainties into a similar array for this:</p> <pre><code>unknown_energies = [\n    (predicted_energies[0], predicted_uncertainty[0]),\n    (predicted_energies[1], predicted_uncertainty[1])\n]\n</code></pre> <p>Next, we can create a function which takes these arrays as inputs and returns which combination of isotopes is our best fit. Before the nested <code>for loop</code> mentioned, we'll want to create an empty array which we can fill with our results. Then, for each choice in <code>peak_one</code>, we'll go through each option in <code>peak_two</code> and select the observed/predicted energies, their uncertainties, and the candidate energies, loading them all into separate arrays which we can then use to calculate \\(\\chi^2\\). We'll load the results into the results array, and then when the loop is completed we will sort it by \\(\\chi^2_{red}\\). Then we can run this function and print out the results!</p> <pre><code>def find_best_isotope_pair(unknown_energies, peak_one, peak_two):\n    results = []\n    for peak1_isotope, peak1_energy in peak_one:\n        for peak2_isotope, peak2_energy in peak_two:\n            observed = np.array([unknown_energies[0][0], unknown_energies[1][0]])\n            uncertainty = np.array([unknown_energies[0][1], unknown_energies[1][1]])\n            expected = np.array([peak1_energy, peak2_energy])\n\n            residuals = (observed - expected) / uncertainty\n            chi2 = float(np.sum(residuals**2))\n            chi2_reduced = chi2 / len(observed)\n\n            results.append((\n                (peak1_isotope, peak2_isotope),\n                (peak1_energy, peak2_energy),\n                chi2,\n                chi2_reduced\n            ))\n    results.sort(key=lambda x: abs(x[3]-1))\n    return results\n\nresults = find_best_isotope_pair(unknown_energies, peak_one, peak_two)\nprint(f'Best fit pair: {results[0][0]}, chi-squared reduced: {results[0][3]}')\nprint(f'Worst fit pair: {results[-1][0]}, chi-squared reduced: {results[-1][3]}')\n</code></pre> <p>Output:</p> <pre><code>Best fit pair: ('Nb-94', 'Zn-65'), chi-squared reduced: 0.72\nWorst fit pair: ('Pm-144', 'Te-121'), chi-squared reduced: 0.11\n</code></pre>"},{"location":"06_chi_squared/#conclusion","title":"Conclusion","text":"<p>In this lesson, we've explored the fundamentals of data fitting. Data fitting is a crucial tool in scientific computation that allows us to extract meaningful patterns, validate hypotheses, and quantify uncertainties in experimental data. Whether you're comparing theory to experiment, identifying trends, or modeling physical systems, fitting lets you move from raw data to interpretable results.</p> <p>We introduced two powerful fitting methods in Python: <code>linregress</code>, a quick and simple tool for linear fits, and <code>curve_fit</code>, a more flexible option for fitting arbitrary functions to data. We also discussed the importance of weighting data points based on their uncertainties, which led us to the reduced \\(\\chi^2\\) test. By applying this test, we demonstrated how to judge our fits using statistical evidence. </p> <p>Altogether, you've now seen how Python tools and statistical reasoning can come together to support robust data analysis. As your datasets grow more complex, these skills will form a foundation for more advanced modeling, error propagation, and hypothesis testing.</p>"}]}